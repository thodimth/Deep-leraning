{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST with Keras\n",
    "\n",
    "## Overview\n",
    "\n",
    "Fashion MNIST is an MNIST like dataset using images of clothing instead of hand-written digits. Each photo is 28x28 grayscale, with 10 different classes. The dataset contains 70000 images with 60000 for training and 10000 for testing.\n",
    "\n",
    "## Download the dataset\n",
    "download from Kaggle (https://www.kaggle.com/zalando-research/fashionmnist)\n",
    "\n",
    "## Libraries Needed\n",
    "We will be using Anaconda (python 3.6 or 3.5) and Keras with TensorFlow backend.\n",
    "\n",
    "- <b>Anaconda</b> (https://www.youtube.com/watch?v=T8wK5loXkXg&t=9s)\n",
    "- <b>TensorFlow</b> (https://www.youtube.com/watch?v=RplXYjxgZbw)\n",
    "- <b>Keras</b> (```pip install keras```)\n",
    "\n",
    "## Models\n",
    "\n",
    "We'll be using models based on examples from Kaggle (https://www.kaggle.com/danialk/range-of-cnns-on-fashion-mnist-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 785)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('fashion_train.csv')\n",
    "test_df = pd.read_csv('fashion_test.csv')\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training and testing data into X (image) and Y (label) arrays\n",
    "\n",
    "train_data = np.array(train_df, dtype='float32')\n",
    "test_data = np.array(test_df, dtype='float32')\n",
    "\n",
    "x_train = train_data[:, 1:] / 255\n",
    "y_train = train_data[:, 0]\n",
    "\n",
    "x_test = test_data[:, 1:] / 255\n",
    "y_test = test_data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training data into train and validate arrays (will be used later)\n",
    "\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets see what the images look like\n",
    "\n",
    "image = x_train[50, :].reshape((28, 28))\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN\n",
    "\n",
    "- <b>Define the model</b>\n",
    "- <b>Compile the model</b>\n",
    "- <b>Fit the model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (48000, 28, 28, 1)\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "x_validate shape: (12000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "im_rows = 28\n",
    "im_cols = 28\n",
    "batch_size = 512\n",
    "im_shape = (im_rows, im_cols, 1)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], *im_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], *im_shape)\n",
    "x_validate = x_validate.reshape(x_validate.shape[0], *im_shape)\n",
    "\n",
    "print('x_train shape: {}'.format(x_train.shape))\n",
    "print('x_test shape: {}'.format(x_test.shape))\n",
    "print('x_validate shape: {}'.format(x_validate.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=im_shape),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(\n",
    "    log_dir=r'logs\\{}'.format('cnn_1layer'),\n",
    "    write_graph=True,\n",
    "    write_grads=True,\n",
    "    histogram_freq=1,\n",
    "    write_images=True,\n",
    ")\n",
    "\n",
    "cnn_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(lr=0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 16s 332us/step - loss: 0.7647 - acc: 0.7485 - val_loss: 0.4800 - val_acc: 0.8326\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 15s 314us/step - loss: 0.4506 - acc: 0.8427 - val_loss: 0.4063 - val_acc: 0.8634\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 15s 316us/step - loss: 0.3931 - acc: 0.8644 - val_loss: 0.3667 - val_acc: 0.8750\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 15s 320us/step - loss: 0.3642 - acc: 0.8746 - val_loss: 0.3504 - val_acc: 0.8822\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 16s 327us/step - loss: 0.3421 - acc: 0.8813 - val_loss: 0.3516 - val_acc: 0.8737\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 16s 330us/step - loss: 0.3279 - acc: 0.8845 - val_loss: 0.3188 - val_acc: 0.8889\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 16s 324us/step - loss: 0.3117 - acc: 0.8912 - val_loss: 0.3050 - val_acc: 0.8952\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 15s 323us/step - loss: 0.2980 - acc: 0.8946 - val_loss: 0.2994 - val_acc: 0.8972\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 16s 327us/step - loss: 0.2875 - acc: 0.8996 - val_loss: 0.2872 - val_acc: 0.9002\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 15s 321us/step - loss: 0.2814 - acc: 0.9007 - val_loss: 0.2898 - val_acc: 0.8972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e2108485f8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(\n",
    "    x_train, y_train, batch_size=batch_size,\n",
    "    epochs=10, verbose=1,\n",
    "    validation_data=(x_validate, y_validate),\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2830\n",
      " test acc: 0.9025\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('test loss: {:.4f}'.format(score[0]))\n",
    "print(' test acc: {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
